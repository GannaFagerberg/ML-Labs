---
title: "Computer Lab 1b - Regularized classification"
subtitle: 'Machine Learning 7.5 credits'
author: "Martin Hyllienmark, Ganna Fagerberg"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

**INSTRUCTIONS**: 

- The sections named Intro do **not** have any problems for you. Those sections contain code used to set up the data and do some initial analysis, so just read and follow along by running each code chunk. 
- Your problems are clearly marked out as Problem 1, Problem 2 etc. You should answer all problems by adding code chunks and text below each question.
- Your submission in Athena should contain two files:
  - This Rmd file with your answers.
  - A PDF version of this file (use the knit to PDF option above).
- You can also write math expression via LaTeX, using the dollar sign, for exampleÂ´ $\beta$.
- You can navigate the sections of this file clicking (Top Level) in the bottom of RStudio's code window.

# Intro1 - Loading packages and data

Loading some packages first. Do `install.packages()` for each package the first time you use a new package.

```{r loading-packages}
suppressMessages(library(dplyr)) # Package for data transformations and tables
library(caret)  # Fitting ML models with CV and more
library(MLeval) # for plotting ROC curves and more
library("RColorBrewer") # for pretty colors
colors = brewer.pal(12, "Paired")[c(1,2,7,8,3,4,5,6,9,10)];
options(repr.plot.width = 12, repr.plot.height = 12, repr.plot.res = 100) # plot size
set.seed(12332)         # set the seed for reproducability
```

The aim of this part of Lab 1 is to build a classification model for predicting if an items get sold at an eBay auction before the auction ends. The dataset contains data from 1000 auctions of collector coins. Let's load the data and have a look.  

```{r load-ebaydata}
eBayData = read.csv('https://github.com/mattiasvillani/MLcourse/raw/main/Data/eBayData.csv', sep = ',')
eBayData = eBayData[-1] # Remove a variable that we will not use.
eBayData['Sold'] = as.factor((eBayData['Sold']==1)) # Changing from 1->TRUE, 0->FALSE
levels(eBayData$Sold) <- c("notsold","sold")
head(eBayData)
```

The response is the last column `Sold` and we will use all the other features except `nBids` to predict if a sale (`Sold=1`) took place. The features are all characteristics of the auction which would be available at the time the auction was announced.

* **PowerSeller** - does the seller sell often at eBay?
* **VerifyID** - is the sellers ID verified by eBay?
* **Sealed** - is the sold coin sealed in an unopened envelope?
* **MinBlem** - does the coin has a minor blemish (determined by a human from pictures and text description)
* **MajBlem** - does the coin has a major blemish (determined by a human from pictures and text description)
* **LargNeg** - does the seller have a large number of negative feedback from previous actions?
* **LogBook** - log of the coin's value according to book of collector coins.
* **MinBidShare** - (standardized) ratio of the seller's reservation price (lowest tolerated selling price) to the book value.

It is always useful to check if the dataset is imbalanced. There are clearly more sold items that unsold, but the data are not catastrophically imbalanced:

```{r check-imbalance}
message(paste("Percentage of auctions where the object was sold:", 100*sum(eBayData['Sold']=="sold")/1000))
```

Split the data using the `createDataPartition` function in `caret` to get 75% of the data for training and 25% for testing. \
The default method in `caret` is stratified sampling, keeping the same fraction of positive and negative observations in both training and test datasets.

```{r training-test-split}
set.seed(123)
inTrain <- createDataPartition(
  y = eBayData$Sold,
  p = .75, # The percentage of data in the training set
  list = FALSE
)
training = eBayData[ inTrain,]
testing  = eBayData[-inTrain,]
message(paste("Percentage of training auctions where the object was sold:", 
              100*sum(training['Sold']=="sold")/dim(training)[1]))
message(paste("Percentage of test auctions where the object was sold:", 
              100*sum(testing['Sold']=="sold")/dim(testing)[1]))
```

Let's get started by with a logistic regression with the following steps:

- fit a logistic regression on the training data using maximum likelihood with the `glm` function from the basic `stats` package in R.
- predict the test data using the rule $\mathrm{Pr}(y=1|\mathbf{x})>0.5 \Rightarrow y=1$ 
- compute the confusion matrix (using `caret`), and the usual measurements of predictive performance for binary data.

```{r fit-glm}
glmFit = glm(Sold ~ ., family = binomial, data = training)
yProbs = predict(glmFit, newdata = testing, type = "response")
threshold = 0.5 # Predict Sold if yProbs>threshold
yPreds = as.factor(yProbs>threshold)
levels(yPreds) <- c("notsold","sold")
confusionMatrix(yPreds, testing$Sold, positive = "sold")
```
#### Problem 1

- 1a) Reconstruct the 2-by-2 confusion matrix for the test data without using a package, i.e. code it up from yPreds and testing$Sold yourself. 
```{r}

df=cbind(as.character(yPreds), as.character(testing$Sold))
df=as.data.frame(df)
names(df)=c("pred", "labels")

TP=sum(df$pred=="sold" & df$labels=="sold")
TN=sum(df$pred=="notsold" & df$labels=="notsold")
FP=sum(df$pred=="sold" & df$labels=="notsold")
FN=sum(df$pred=="notsold" & df$labels=="sold")

conf_matrix=matrix(c(TN,FN, FP, TP),ncol=2,byrow=TRUE)
colnames(conf_matrix) <- c("notsold_actual","sold_actual")
rownames(conf_matrix) <- c("notsold_pred","sold_pred")
conf_matrix


```

- 1b) Use the confusion matrix in 1a) to compute the accuracy, sensitivity and specificity and the classifier.

```{r}
message(paste("Accuracy:", 
              (round(((TP+TN)/(TP+TN+FN+FP)),4))))
message(paste("Sensitivity:", 
              (round((TP/(TP+FN)),4))))
message(paste("Specificity:", 
              (round((TN/(TN+FP)),4))))

```


- 1c) Compute the ROC curve from the above fitted glm model and plot it. No packages allowed.

```{r}

# Alternative 1
yProbs = predict(glmFit, newdata = testing, type = "response")
sens<-c()
spec<-c()
for (i in seq(0,1,length=1000)){ 
yPreds = as.factor(yProbs>i)
levels(yPreds) <- c("notsold","sold")
sens<-rbind(sens,confusionMatrix(yPreds, testing$Sold, positive = "sold")$byClass[1])
spec<-rbind(spec,confusionMatrix(yPreds, testing$Sold, positive = "sold")$byClass[2])
}
plot(1-spec,sens, type="s", ylab = "True positive rate", xlab = "False positive rate")
lines(seq(0,1,length=1000),seq(0,1,length=1000))

# Sanity check
library(ROCR)
pred = predict(glmFit,testing,type="response")
pred = prediction(pred, testing$Sold) #PRED INSTEAD OF YPROB
roc = performance(pred,"tpr","fpr")
plot(roc, colorize = T, lwd = 2)
abline(a = 0, b = 1) 

```

```{r }
# Alternative 2
scores=predict(glmFit, newdata = testing, type = "response") #predicted probabilities
df1=cbind(df, scores)
df1 <- df1[order(-scores),] #order the probabilities

#Compute the ROC curve
TPR=cumsum(df1$labels=="sold")/sum(df1$labels=="sold")
FPR=cumsum(df1$labels=="notsold")/sum(df1$labels=="notsold")

# Plot the ROC curve
plot(FPR, TPR, type="l", main="ROC curve")
lines(x = c(0,100), y = c(0,100))


```



# Intro2 - Using the caret package

Let's use the `caret` package to fit a **elastic net** classifier where the two hyperparameters $\alpha$ and $\lambda$ are chosen by 10-fold cross-validation.

```{r}
cv10 <- trainControl(
  method = "cv", 
  number = 10, # number of folds
  classProbs = TRUE, 
  summaryFunction = twoClassSummary, # Standard summary for binary outcomes
  savePredictions = TRUE # Important, otherwise MLeval below will not work.
)
glmnetFit <- train(
  Sold ~ .,
  data = training,
  method = "glmnet",
  preProc = c("center", "scale"), # the covariates are centered and scaled
  tuneLength = 10, # The number of tuning parameter values to try
  trControl = cv10,
  metric = "ROC"
)

```

The `MLeval` package can be used to plot ROC curves from model fit with `caret`. By just supplying the fitted model object `glmnetFit` to the function we get the cross-validated results from the training data, i.e. this evaluation is not evaluating against the `testing` data. See below for how to evaluate on the testing data using the `MLeval` package.

```{r ROC-glmnet}
glmnetEval = evalm(glmnetFit, plots='r', rlinethick=0.8, fsize=8, silent=T) # plots='r' gives ROC
```

The optimal model returned in the caret object `glmnetFit` can be directly used for prediction of the test data. The `MLeval` package can again be used for the evaluation, this time on the `testing` data.

```{r glmnet-mleval}
yPreds = predict(glmnetFit, newdata = testing)
yProbs = predict(glmnetFit, newdata = testing, type = "prob")
confusionMatrix(yPreds, testing$Sold, positive = "sold")
res = evalm(data.frame(yProbs, testing$Sold), plots='r', rlinethick=0.8, fsize=8, silent=T)
```



#### Problem 2

Fit a random forest using the `rf` package in `caret` to the training data with tuning parameters chosen by 10-fold cross-validation. Plot the ROC curve and compute AUC for the `testing` data using the package `MLeval`. Compute also the accuracy and recall on the `testing` data.

```{r random-forest-fit}
set.seed(123)

rf_mod <- train(
  Sold ~ .,
  data = training,
  method = "rf",
  ntree = 150,
  preProc = c("center", "scale"),
  tuneLength = 7, 
  trControl = cv10,
  metric = "ROC"
)


plot(rf_mod)
message(paste("The optimal number of variables 
              tried at each split is:", 
              (rf_mod$bestTune[[1]])))


# Compute the ROC curve

# Create the dataset
yProbs_rf = predict(rf_mod, newdata = testing, type = "prob")
df2=cbind(yProbs_rf, testing$Sold)
names(df2)=c("notsold", "sold", "labels")
df2$Group=rep("rf", nrow(df2))

# The ROC plot
ROC_rf <- evalm(df2, plots='r',rlinethick=0.8,fsize=8,bins=8,silent=T)
AUC_rf=unlist(ROC_rf$stdres)[13]; 
message(paste("AUC-ROC :",
              AUC_rf))

# Accuracy and recall
yPreds_rf = predict(rf_mod, newdata = testing)
Accuracy_rf=round((confusionMatrix(yPreds_rf, testing$Sold, positive = "sold")$overall[1]),4) 
message(paste("Accuracy :", 
              (Accuracy_rf)))

Recall_rf=round((confusionMatrix(yPreds_rf, testing$Sold, positive = "sold")$byClass[1]),4)
message(paste("Recall :", 
              (Recall_rf)))

```


#### Problem 3

Same as Problem 2, but using the k-nearest neighbor classifier `knn` in caret, with $k$ chosen by 10-fold cross-validation between $k=1$ and $k=10$. You need to use the `tuneGrid` option instead of the `tuneLength` option in `caret` for this.

```{r kNN-classifier}
set.seed(123)

knn_mod <- train(
  Sold ~ .,
  data = training,
  method = "knn",
  preProc = c("center", "scale"),
  tuneGrid = expand.grid(k = c(1:10)), 
  trControl = cv10,
  metric = "ROC"
)


plot(knn_mod)
message(paste("The optimal number of k-neighbors is :", 
              (knn_mod$bestTune[[1]])))

# Compute the ROC curve

# Create the dataset
yProbs_knn = predict(knn_mod, newdata = testing, type = "prob")
df3=cbind(yProbs_knn, testing$Sold)
names(df3)=c("notsold", "sold", "labels")
df3$Group=rep("knn", nrow(df3))

# The ROC plot
ROC_knn <- evalm(df3,plots='r',rlinethick=0.8,fsize=8,bins=8, silent=T)
AUC_knn=unlist(ROC_rf$stdres)[13]; 
message(paste("AUC-ROC :", 
              (AUC_knn)))

# Accuracy and recall
yPreds_knn = predict(knn_mod, newdata = testing)
Accuracy_knn=round((confusionMatrix(yPreds_knn, testing$Sold, positive = "sold")$overall[1]),4) 
message(paste("Accuracy :", 
              (Accuracy_knn)))

Recall_knn=round((confusionMatrix(yPreds_knn, testing$Sold, positive = "sold")$byClass[1]),4)
message(paste("Recall :", 
              (Recall_knn)))


```



#### Problem 4

Same as Problem 2, but using stochastic gradient boosting through the `gbm` package in `caret`. There are four tuning parameters here, but you can keep two of them fixed in the training: `n.trees = 100` and `n.minobsinnode = 10`. You need to use the `tuneGrid` option instead of the `tuneLength` option in `caret` for this. Just before the tuneGrid argument in the train function, you can add the argument `verbose = FALSE` to avoid unnecessary messages from being printed during the training process.

```{r}

library(gbm)
set.seed(123)
gbm_mod <- train(
  Sold ~ .,
  data = training,
  method = "gbm",
  preProc = c("center", "scale"), # the covariates are centered and scaled
  verbose = FALSE,
  tuneGrid = expand.grid(interaction.depth = 1:10,
                    n.trees = 100, 
                    shrinkage = c(.001, .01, .1),
                    n.minobsinnode = 10), 
  trControl = cv10,
  metric = "ROC"
)

plot(gbm_mod)
message(paste("The optimal interaction.depth is :",
              (gbm_mod$bestTune[[2]])))

message(paste("The optimal shrinkage parameter is :",
              (gbm_mod$bestTune[[3]])))

# Compute the ROC curve

# Create the dataset
yProbs_gbm = predict(gbm_mod, newdata = testing, type = "prob")
df4=cbind(yProbs_gbm, testing$Sold)
names(df4)=c("notsold", "sold", "labels")
df4$Group=rep("gbm", nrow(df4))

# The ROC plot
ROC_gbm <- evalm(df4,plots='r',rlinethick=0.8,fsize=8,bins=8, silent=T)
AUC_gbm=unlist(ROC_gbm$stdres)[13]; 
message(paste("AUC-ROC :", 
              (AUC_gbm)))

# Accuracy and recall
yPreds_gbm = predict(gbm_mod, newdata = testing)
Accuracy_gbm=round((confusionMatrix(yPreds_gbm, testing$Sold, positive = "sold")$overall[1]),4) 
message(paste("Accuracy :", 
              (Accuracy_gbm)))

Recall_gbm=round((confusionMatrix(yPreds_gbm, testing$Sold, positive = "sold")$byClass[1]),4)
message(paste("Recall :", 
              (Recall_gbm)))





```


#### Problem 5

Same as Problem 2, but using any model from `caret` that you are curious about. Here is the [list of models](https://topepo.github.io/caret/available-models.html) in `caret`.

```{r}

# The neural network model 
garbage=capture.output(
nn_mod <- train(
  Sold ~ .,
  data = training,
  method = "nnet",
  preProc = c("center", "scale"),
  verbose = FALSE,
  tuneGrid=expand.grid(size=c(10), decay=c(0.1)), 
  trControl = cv10,
  metric = "ROC"
))

# Create the dataset
yProbs_nn = predict(nn_mod, newdata = testing, type = "prob")
df5=cbind(yProbs_nn, testing$Sold)
names(df5)=c("notsold", "sold", "labels")
df5$Group=rep("nn", nrow(df5))


# The ROC plot
ROC_nn <- evalm(df5,plots='r',rlinethick=0.8,fsize=8,bins=8, silent=T)
AUC_nn=unlist(ROC_nn$stdres)[13]; 
message(paste("AUC-ROC :", 
              (AUC_nn)))

# Accuracy and recall
yPreds_nn = predict(nn_mod, newdata = testing)
Accuracy_nn=round((confusionMatrix(yPreds_nn, testing$Sold, positive = "sold")$overall[1]),4) 
message(paste("Accuracy :", 
              (Accuracy_nn)))

Recall_nn=round((confusionMatrix(yPreds_nn, testing$Sold, positive = "sold")$byClass[1]),4)
message(paste("Recall :", 
              (Recall_nn))) 

```



#### Problem 6

Plot the ROC curves on the `testing` data from models fit under Problem 2-6 in a single plot. [Hint: the `evalm` function in the `MLeval` package can plot for more than one model.]


```{r}
# Combine the datasets from Problem 2-4
df_all=rbind(df2,df3,df4,df5)

# The ROC plots
ROC_all <- evalm(df_all,plots='r',rlinethick=0.8,fsize=8,bins=8, silent=T)

```


