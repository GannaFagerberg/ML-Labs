---
title: "Lab 1a - Regularized regression"
subtitle: 'Machine Learning 7.5 credits'
author: "Martin Hyllienmark, Ganna Fagerberg"
output:
  html_document:
    df_print: paged
  pdf_document: 
    latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message=FALSE}
suppressMessages((library(dplyr)))
library(tidyverse)
library(caret)
library(splines)
library(ggplot2)
library(glmnet)
library(Hmisc)
library(tree)
library(randomForest)
library(xgboost)
library("RColorBrewer") 
colors = brewer.pal(12, "Paired")[c(1,2,7,8,3,4,5,6,9,10)]
set.seed(123342)         
```
Let's load the data, make some transformations and have look at the raw data.

```{r read-bike-share-data}
bikes = read.csv("https://github.com/mattiasvillani/MLcourse/raw/main/Data/BikeShareData/hour.csv")
bikes$dteday = as.Date(bikes$dteday) # convert date column to proper date format
bikes$logrides = log(bikes$cnt)      # we model the log(number of rides) as response
bikes$hour = bikes$hr/23             # hour of the day: midnight is 0, 11 PM is 1.
head(bikes)
```

We will start by training on only 2 months, February and March of 2011, and only using the variable `hour` as predictor for `logrides`. As seen in the figure below, the expected the number of rides over the day seems to peak in the morning (8 AM is $8/23  \approx 0.35$) and after work (6 PM is $18/23  \approx 0.78$).

```{r setup-small-training-data}
bikesTrain = bikes[bikes$dteday >= as.Date("2011-02-01") & 
                     bikes$dteday <= as.Date("2011-03-31"),] #training set

yTrain = bikesTrain$logrides #response variable

plot(bikesTrain$hour, bikesTrain$logrides, xlab = "hour", ylab = "logrides", 
     col = colors[2], cex = 0.5) 
```



#### Problem 1 - Polynomial regression
Code everything from scratch on this problem, no `lm()`  or anything. You **can** however use the functions defined in this note book, for example `PolyPlotFit`. Do this:

- Fit a polynomial regression of `logRides` against the covariate `hour` to the training data (Feb 1, 2011 - March 31, 2011) using a polynomials of order 8. Plot the fit of the model overlayed on the scatter of training data.
- Fit polynomials with order varying between 1 and 10 on the training data. Plot the training RMSE as a function of the polynomial order.
- For each polynomial order between 1 and 10, use the trained model to predict the `logRides` on the test set consisting of the following 2 months between April 1, 2011 - May 31, 2011. Compute the test RMSE for each polynomial order and plot it in the same plot as the training RMSE.
- Comment on the difference of the RMSE on the training and test data: are we overfitting or underfitting the data? Other explanation of the results?


```{r setting-up-fuctions}
# Function that computes the basis function 
# for a vector of x-values. 
PolyMatrix <- function(x, order){
    X = cbind(1,x)
    if (order==1){return(X)}
    for (k in 2:order){
        X = cbind(X, x^k)
    } 
    return(X)
}

# Function that trains a polynomial model, 
# computes predictions over fine grid of values xGrid 
# and plots the fit and training data.
PolyPlotFit <- function(x, y, order, xGrid){
    X = PolyMatrix(x, order)
    betaHat = solve(crossprod(X),crossprod(X,y))
    Xgrid = PolyMatrix(xGrid, order)
    yFit = Xgrid%*%betaHat
    plot(x, y, pch = 16, cex = 0.5)
    lines(xGrid, yFit, col = colors[2], lwd = 2)
    legend(x = "topleft", inset=.05, legend = c("Data", "Fit"),  
       lty = c(NA, 1), lwd = c(2, 2), pch = c(16, NA),
       col = c("black", colors[2]))
}

```



- Fit a polynomial regression of `logRides` against the covariate `hour` to the training data (Feb 1, 2011 - March 31, 2011) using a polynomial of order 8. Plot the fit of the model overlayed on the scatter of training data.


```{r fit-polynomial-order-eight}

# Function that computes RMSE 
rmse = function(actual, predicted) {
  sqrt(mean((actual - predicted) ^ 2))
}

# Training a polynomial model of order 8
order=8
xGrid = seq(0, 1, length = 100)
XTrain8 = PolyMatrix(bikesTrain$hour, order)
yTrain8 = bikesTrain$logrides

betaHat8 = solve(crossprod(XTrain8),crossprod(XTrain8,yTrain8))
yFit8 = XTrain8%*%betaHat8

RMSEtrain8=round(rmse(yTrain8,yFit8),4)
message(paste("Training RMSE: ", RMSEtrain8))

# Plotting the fit of the model over the training data

PolyPlotFit(bikesTrain$hour, bikesTrain$logrides, order, xGrid)

```

- Fit polynomials with order varying between 1 and 10 on the training data. Plot the training RMSE as a function of the polynomial order.
- For each polynomial order between 1 and 10, use the trained model to predict the `logRides` on the test set consisting of the following 2 months between April 1, 2011 - May 31, 2011. Compute the test RMSE for each polynomial order and plot it in the same plot as the training RMSE.
- Comment on the difference of the RMSE on the training and test data: are we overfitting or underfitting the data? Other explanation of the results?

```{r setup-small-test-data}
# Setting up the test dataset, April 1, 2011 - May 31, 2011.
bikesTest = bikes[bikes$dteday >= as.Date("2011-04-01") & 
                    bikes$dteday <= as.Date("2011-05-31"),]
yTest = bikesTest$logrides

```


```{r fit-polynomials-orders-one-to-ten}
orders<-10  # the max number of orders 
order<-vector("list",orders) #an empty list to store the results
plot_rmse_training<-c() #an empty vector for the loop
plot_rmse_testing<-c() #an empty vector for the loop

# Computing betahat, fitted values, predicted values and RMSE
for (i in 1:orders){
    order[[i]]<-vector("list",8)
    names(order[[i]])<-c("order","XTrain","betahat","yFit","XTest","yPred","RMSEtrain","RMSEtest")
    order[[i]]$order<-i
    order[[i]]$XTrain<- PolyMatrix(bikesTrain$hour, i)
    order[[i]]$betahat<-solve(crossprod(order[[i]]$XTrain),crossprod(order[[i]]$XTrain,yTrain))
    order[[i]]$yFit = order[[i]]$XTrain%*%order[[i]]$betahat
    order[[i]]$XTest<-PolyMatrix(bikesTest$hour, i)
    order[[i]]$yPred<-order[[i]]$XTest%*%order[[i]]$betahat
    order[[i]]$RMSEtrain = sqrt(sum((yTrain-order[[i]]$yFit)^2)/length(yTrain))
    order[[i]]$RMSEtest = sqrt(sum((yTest-order[[i]]$yPred)^2)/length(yTest))
    plot_rmse_training[i]<-order[[i]]$RMSEtrain
    plot_rmse_testing[i]<-order[[i]]$RMSEtest
}

a_list = list(plot_rmse_training,plot_rmse_testing) # RMSE for all orders (both test and training data)

a_df = do.call("rbind", lapply(a_list, function(x) data.frame(RMSE = x, order = seq_along(x)))) # add order to the list

ID_options = c("train","test") 
a_df$ID = rep(ID_options, sapply(a_list, length)) #add information from ID_options to the RMSE list

# Plotting the training and test RMSE results
ggplot(a_df, aes(y = RMSE, x = order, color = ID)) + geom_point()+
  scale_x_continuous(breaks=seq(1, 10, 1))+scale_y_continuous(breaks=seq(0.05, 1.5, 0.1))+ geom_line()

```


It seems that we underfit the model up to order 5 and we start to overfit it with the subsequent increase in the model's flexibility. It's not obvious which model can be deemed as best. There is a gap between the two curves (training and test RMSE), which is increasing with the order of the model (though it stabilizes somewhat after order 8).  It may indicate that we need to include more features into the model or come up with a more complex model.





#### Problem 2 - Spline regression with L2 regularization

Use the package `glmnet` to fit a spline regression. Use L2-regularization and find the optimal $\lambda$ by $10$-fold cross-validation on the training data using the one-standard deviation rule (lambda.1se). Use the `splines` package in R to create natural cubic splines basis functions with 10 degrees of freedom, i.e. use the `ns()` function with df=10 as input argument. Note that `glmnet` does not allow R model formulas as inputs and also wants matrices as inputs rather than dataframes. 
Compute the RMSE in training (Feb-March) and test (April-May).


```{r L2-regularization}

set.seed(123342)

# Function to create datasets with splined variables
df_splin_2=function(predictor, response){
  hour_splin <- ns(predictor, df = 10)
  df_splin <- as.data.frame(hour_splin)
  hour=predictor #include the linear term
  df_splin=cbind(hour, df_splin)
  names(df_splin)=c("hour", "S1","S2","S3","S4","S5","S6","S7","S8","S9","S10")
  df_splin <- as.matrix(df_splin)
  return(df_splin)
}

# Function to compute RMSE  
rmse_foo=function(model, dat, actual){
  pred = predict(model, dat)
  rmse=rmse(actual, pred) 
  return(rmse)
}

# Creating training and test datasets with splined variables
df_train_2=df_splin_2(bikesTrain$hour, yTrain) 
df_test_2=df_splin_2(bikesTest$hour, yTest)

# Fitting a spline regression with L2-regularization
cv_fit_L2 = cv.glmnet(df_train_2, yTrain, alpha = 0, standardize=T) 


# Obtaining the optimal lambda 
plot(cv_fit_L2) 
opt_lambda = round((cv_fit_L2$lambda.1se),4) 
message("The optimal lambda is ", opt_lambda)

# Computing the training RMSE 
rmse_tr_2=round(rmse_foo(cv_fit_L2, df_train_2, yTrain),4)
message("The training RMSE is ", rmse_tr_2)

#Computing the test RMSE 
df_test_2=df_splin_2(bikesTest$hour, yTest)
rmse_ts_2=round(rmse_foo(cv_fit_L2, df_test_2, yTest),4)
message("The test RMSE is ", rmse_ts_2)
```



#### Problem 3 - Spline regression with L1 regularization
Repeat Problem 2, this time using L1 regularization.

```{r L1-regularization}
set.seed(123342)

# Fitting a spline regression with L1-regularization
cv_fit_L1 = cv.glmnet(df_train_2, yTrain, alpha = 1, standardize=T) #set alpha=1 for Lasso

# Obtaining the optimal lambda 
plot(cv_fit_L1)
opt_lambda_L1 = round((cv_fit_L1$lambda.1se), 4)
message("The optimal lambda is ", opt_lambda_L1)

#Computing the training RMSE  
rmse_tr_3=round((rmse_foo(cv_fit_L1, df_train_2, yTrain)),4)
message("The training RMSE is ", rmse_tr_3)

#Compute the test RMSE
rmse_ts_3=round((rmse_foo(cv_fit_L1, df_test_2, yTest)),4)
message("The test RMSE is ", rmse_ts_3)


```




#### Problem 4 - Spline regression with L1-regularization with more covariates

Use `glmnet` to estimate an L1 regularized regression for the following regression model expressed for clarity as an R formula:

logrides ~ s(hour) + yr + holiday + workingday + temp + atemp + hum + windspeed + weekdayDummies + seasonDummies + weatherDummies,

where 

- s(hour) are spline terms, so that s(hour) means adding all the splines covariates to the model, one for each knot (in addition to the linear term)
- weekdayDummies, seasonDummies and weatherDummies, each means to add all the one-hot covariates for each these three effects.

Compute the RMSE on the training (Jan 1, 2011 - May 31, 2012) and the test data (June 1, 2012- Dec 31, 2012). Which three covariates seem to be most important in the training data?

```{r one-hot-function}
onehot <- function(x){
    levels = sort(unique(x))
    onehotMatrix = matrix(0, length(x), length(levels)-1)
    count = 0
    for (level in levels[-1]){
        count = count + 1
        onehotMatrix[x == level, count] = 1
    }
    return(onehotMatrix)
}
```

```{r constructing-one-hot-bikes-data}
weatherOneHot = data.frame(onehot(bikes$weathersit))
names(weatherOneHot) <- c("cloudy", "lightrain","heavyrain")
bikes = cbind(bikes, weatherOneHot)

weekdayOneHot = data.frame(onehot(bikes$weekday))
names(weekdayOneHot) <- c("mon","tue","wed","thu","fri","sat")
bikes = cbind(bikes, weekdayOneHot)

seasonOneHot = data.frame(onehot(bikes$season))
names(seasonOneHot) <- c("summer", "fall","winter")
bikes = cbind(bikes, seasonOneHot)

head(bikes) # always look at the to see that we didn't mess it up.
```


```{r spline-regression-more-covariates}

# Function to include splines in a dataset
df_splin=function(predictor, dat){
  hour_splin <- ns(predictor, df = 10)
  df <- as.data.frame(hour_splin)
  names(df)=c("S1","S2","S3","S4","S5","S6","S7","S8","S9","S10" )
  df <- cbind(dat, df)
  df<-df[,c(-1,-3, -5,-6,-8,-10,-15:-17)]
  return(df)
}

# Adding splined variables to the bikes dataset
bikes_4=df_splin(bikes$hour, bikes) 


# Creating the training set
bikes_train = bikes_4[bikes_4$dteday >= as.Date("2011-01-01") & 
                        bikes_4$dteday <= as.Date("2012-05-31"),] 

y_train=bikes_train$logrides #response variable
df_train_4=bikes_train[,c(-1, -9)] #remove redundant variables 
df_train_4=as.matrix(df_train_4)# matrix format


# Creating the test set
bikes_test = bikes_4[bikes_4$dteday >= as.Date("2012-06-01") & 
                       bikes_4$dteday <= as.Date("2012-12-31"),] 

y_test=bikes_test$logrides

df_test_4=bikes_test[,c(-1, -9)]
df_test_4=as.matrix(df_test_4)

# Fitting a spline regression with L1-regularization 
set.seed(123342)
cv_fit_4 = cv.glmnet(df_train_4, y_train ,alpha = 1)

# Computing the optimal lambda
plot(cv_fit_4)
opt_4 = round((cv_fit_4$lambda.1se),4)
message("The optimal lambda is ", opt_4)


# Computing the training RMSE 
rmse_tr_4=round((rmse_foo(cv_fit_4, df_train_4, y_train )),4)
message("The training RMSE ", rmse_tr_4)

# Computing the test RMSE
rmse_ts_4=round((rmse_foo(cv_fit_4, df_test_4, y_test )),4)
message("The test RMSE is ", rmse_ts_4)

# Which three covariates seem to be most important in the training data?
coef(cv_fit_4)
```

The three most important covariates are: atemp, hour, splines (namely, the first and the ninth spline terms). This is also confirmed by the Lasso path plot (with the largest weights attributed to the first and ninth spline terms).

```{r lasso-path-plot}
# The Lasso pass plot
plot(cv_fit_4$glmnet.fit, 
     "lambda", label=T)
```




#### Problem 5 - Time series effects in a regression
So far we have ignored that the data is a time series. Let us now check if the residuals are autocorrelated, and then try to improve on the model by adding time series effects in the regression. \

Do the following steps:

-Plot the autocorrelation function for the residuals in the training data from the previously fitted L1-regularized regression (hint: acf()). Comment. \

```{r acf-plot}
# Computing the residuals
resid_5=y_train-predict(cv_fit_4, df_train_4)

# producing the ACF plot 
acf_5=acf(resid_5, lag.max = 100)

```


The residuals exhibit a persistent autocorrelation pattern. The pattern seems to follow some seasonal trends. One suggestion is that the pattern depends on a 24-hours day cycle (hour-to-hour cycle).



- Plot the actual time series for the last 24*7 observations in the dataset (the last week) and the prediction for those values in the same graph. Here you should plot the data on the original scale, i.e. plot exp(logrides) and exp() of the predictions. \

```{r actual-vs-predicted-ts}

y_actual=as.vector(exp(tail(y_test,(24*7-1)))) #actual observations
y_pred=tail(as.vector(exp(predict(cv_fit_4, df_test_4))),(24*7-1)) #predicted values

date=tail(as.Date(bikes_test$dteday),(24*7-1)) #date variable 
hr=tail((bikes_test$hour)*23,(24*7-1)) #hour variable 

df_plot_5b=data.frame(date,hr,y_actual, y_pred) #combine all the relevant variables 

new_date=as.POSIXct(paste(df_plot_5b$date, df_plot_5b$hr), format="%Y-%m-%d %H") #format the date and hour variables accordingly
df_plot_5b$date_hour=new_date 

#Plot the predicted values against the observed values
plot(df_plot_5b$date_hour, 
     df_plot_5b$y_actual, 
     pch = 16,
     lwd = 2, 
     cex = 0.5, 
     type = 'l', 
     col = colors[4], 
     ylab = "# bikes rides",
     xlab = "Date")
lines(df_plot_5b$date_hour, 
      df_plot_5b$y_pred, 
      col = colors[2], 
      lwd = 2)
legend(x = "topleft", 
       inset=.05, 
       legend = c("Actual", "Predicted"),  
       lty = c(1, 1), 
       lwd = c(2, 2), 
       pch = c(NA, NA),
       col = c(colors[4], colors[2]))

```


- Add time series effects by adding the first four hourly lags and the 24th hourly lag to the set of covariates. (hint: lag() and note that you loose observations when taking lags). Fit the L1-regularized regression with all previous covariates and the new time lags. Compute RMSE in training and in test.  \

```{r ts-L1-regularization}
set.seed(123342)

# Adding lagged covariates to the dataset
lag_hour1=Lag(bikes_4$logrides, shift=1)
lag_hour2=Lag(bikes_4$logrides, 2)
lag_hour3=Lag(bikes_4$logrides, 3)
lag_hour4=Lag(bikes_4$logrides, 4)
lag_hour24=Lag(bikes_4$logrides, 24)

lag_comb=as.data.frame(cbind(lag_hour1,lag_hour2,lag_hour3,lag_hour4,lag_hour24))
lag_comb = lag_comb[-1:-24,] # delete the missing values
df_lag=cbind(bikes_4[-1:-24,],lag_comb)

# Adding the lagged variables to the training set
lag_train = df_lag[df_lag$dteday >= as.Date("2011-01-01") & 
                     df_lag$dteday <= as.Date("2012-05-31"),] 
y_tr_l=lag_train$logrides # actual y-train observations

lag_tr=lag_train[,c(-1, -9)] #delete redundant variables
lag_tr=as.matrix(lag_tr)

# Adding the lagged variables to the test set
lag_test = df_lag[df_lag$dteday >= as.Date("2012-06-01") & 
                    df_lag$dteday <= as.Date("2012-12-31"),] 

y_ts_l=lag_test$logrides #actual y-test observations

lag_ts=lag_test[,c(-1, -9)]
lag_ts=as.matrix(lag_ts)

# Fitting a spline regression, L1
cv_L1_5 = cv.glmnet(lag_tr, y_tr_l ,alpha = 1, standardize=T)

# Obtaining the optimal lambda
plot(cv_L1_5)
opt_L1_5 = round((cv_L1_5$lambda.1se),4)
message("The optimal lambda is ",opt_L1_5)

# Obtaining the beta coefficients 
coef(cv_L1_5, opt_L1_5)

# Computing the training RMSE
rmse_tr_5=round((rmse_foo(cv_L1_5, lag_tr, y_tr_l)),4)
message("The training RMSE is ",rmse_tr_5)

# Computing the test RMSE 
rmse_ts_5=round((rmse_foo(cv_L1_5, lag_ts, y_ts_l)),4)
message("The test RMSE is ",rmse_ts_5)

```


```{r model-comparison}
# Sanity check
# Comparing the the models with and without the lagged terms
df_comp = data.frame(
  Model = c("Splies", "Splines_L"),
  Number=c( Number=c("4", "5")),
  Train_rmse = c(rmse_tr_4,rmse_tr_5),
  Test_rmse = c(rmse_ts_4, rmse_ts_5))
df_comp

# Checking the residuals plot, ACF
r=y_tr_l-predict(cv_L1_5, lag_tr)

# Producing the ACF plot 
acf=acf(r, lag.max = 100)
```

Note that we have substantially reduced the test error of the model that accounts for the times series nature of the data compared to the model that ignores it. Also, we have markedly reduced the autocorrelation pattern in the residuals.

- Plot the actual time series for the last $24 * 7 = 168$ observations in the dataset (the last week) and the prediction for those values in the same graph.

```{r actual-to-predicted-lagged}

y_pred_lag=tail(as.vector(exp(predict(cv_L1_5, lag_ts))),(24*7-1))


plot(df_plot_5b$date_hour, 
     df_plot_5b$y_actual, 
     pch = 16,
     lwd = 2, 
     cex = 0.5, 
     type = 'l', 
     col = colors[4], 
     ylab = "exp(logrides)",
     xlab = "Date")
lines(df_plot_5b$date_hour, 
      y_pred_lag, 
      col = colors[2], 
      lwd = 2)
legend(x = "topleft", 
       inset=.05, 
       legend = c("Actual", "Predict"),  
       lty = c(1, 1), 
       lwd = c(2, 2), 
       pch = c(NA, NA),
       col = c(colors[4], colors[2]))
```




#### Problem 6 - Regression trees
We will now fit a regression tree.

- Use the training dataset created in problem 5 c, this time to fit regression trees from the `tree` package. Compute RMSE for the training and test sets. Use the default settings when reporting the results, but feel free to experiment with the settings to see the effect of changing them. Plot the fitted model to show the tree structure.

```{r regression-tree}
set.seed(123342)

# Preparing the training dataset
df_tree_tr=as.data.frame(cbind(y_tr_l,lag_tr))

# Preparing the test dataset
df_tree_ts=data.frame(lag_ts)

# Fit the tree model
tree_mod = tree(y_tr_l ~ ., data = df_tree_tr)
summary(tree_mod)
message("The number of terminal nodes used is ", summary(tree_mod)$size)

# Computing the training RMSE
rmse_tr_6=round((rmse_foo(tree_mod,df_tree_tr,y_tr_l)),4)
message("The training RMES is ", rmse_tr_6)

# Computing the test RMSE
rmse_ts_6=round((rmse_foo(tree_mod,df_tree_ts,y_ts_l)),4)
message("The test RMSE is ", rmse_ts_6)

# Plot the regression tree
plot(tree_mod)
text(tree_mod, pretty = 0)
title(main = "Unpruned Regression Tree")

```
```{r regression-tree-best-tune}
# Using cross-validation to select a good pruning of the tree.
tree_mod.cv = cv.tree(tree_mod)
plot(tree_mod.cv$size, sqrt(tree_mod.cv$dev / nrow(df_tree_tr)), type = "b", xlab = "Tree Size", ylab = "CV-RMSE")
```

The size 8 of the tree does have the lowest RMSE. We will however try pruning the tree with size 6, which seems to perform slightly worse in terms of RMSE, and size 4 for comparison purposes. 

```{r regression-tree-experiments}
# Experimenting with the number of trees

# Prune4
tree_mod_prune4 = prune.tree(tree_mod, best = 4)
summary(tree_mod_prune4)

plot(tree_mod_prune4)
text(tree_mod_prune4, pretty = 0)
title(main = "Pruned Regression Tree, 4 terminal nodes")

# Computing the training RMSE
rmse_tr_6_4=round((rmse_foo(tree_mod_prune4,df_tree_tr,y_tr_l)),4)
message("The training RMES (prune=4) is ", rmse_tr_6_4)

# Computing the test RMSE
rmse_ts_6_4=round((rmse_foo(tree_mod_prune4,df_tree_ts,y_ts_l)),4)
message("The test RMSE (prune=4) is ", rmse_ts_6_4)

#Prune7
tree_mod_prune7 = prune.tree(tree_mod, best = 7)
summary(tree_mod_prune7)

plot(tree_mod_prune7)
text(tree_mod_prune7, pretty = 0)
title(main = "Pruned Regression Tree, 7 terminal nodes")

# Computing the training RMSE
rmse_tr_6_7=round((rmse_foo(tree_mod_prune7,df_tree_tr,y_tr_l)),4)
message("The training RMES (prune=7) is ", rmse_tr_6_7)

# Computing the test RMSE
rmse_ts_6_7=round((rmse_foo(tree_mod_prune7,df_tree_ts,y_ts_l)),4)
message("The test RMSE (prune=7) is ", rmse_ts_6_7)

```

- Plot the actual time series for the last  24∗7=168 observations in the dataset (the last week) along with the predictions from the tree model.

```{r plot_actual-to-predicted-regr-tree}
y_pred_tree=tail(as.vector(exp(predict(tree_mod, df_tree_ts))),(24*7-1))

plot(df_plot_5b$date_hour, 
     df_plot_5b$y_actual, 
     pch = 16,
     lwd = 2, 
     cex = 0.5, 
     type = 'l', 
     col = colors[4], 
     ylab = "exp(logrides)",
     xlab = "Date")
lines(df_plot_5b$date_hour, 
      y_pred_tree, 
      col = colors[2], 
      lwd = 2)
legend(x = "topleft", 
       inset=.05, 
       legend = c("Actual", "Predict"),  
       lty = c(1, 1), 
       lwd = c(2, 2), 
       pch = c(NA, NA),
       col = c(colors[4], colors[2]))
```



#### Problem 7 - Random forest
Repeat Problem 6, this time using the random forest regression model from the `randomForest` package. No need to plot the trees or the forest, however.

The default setting of the randomForest function is to train 500 trees. You can speed up the function by specifying a lower number of trees with the argument ntree.

The argument sampsize allows you to train each tree using only a sample of the training set. A smaller sample size speeds up the function.

You can test a few different combinations of ntree and sampsize. Is it better in terms of accuracy to train many trees, each with a smaller sample size, or to train fewer trees each with the full training set?

```{r random-forest-model}
set.seed(123342)

# Fitting a random forest model
rf_mod = randomForest(y_tr_l ~ ., data = df_tree_tr, 
                      importance = TRUE, ntrees = 500)

print(rf_mod)
plot(rf_mod)

# Summary of the importance of the variables 
importance(rf_mod, type = 1)


# Computing the test and training RMSE
tr_1=round((rmse_foo(rf_mod, df_tree_tr, y_tr_l)),4) 
message("The training RMSE, default seetings: ", tr_1)

ts_1=round((rmse_foo(rf_mod, df_tree_ts, y_ts_l)),4)
message("The test RMSE, default seetings: ", ts_1)

# Plotting the actual observations versus the predicted for the last week
y_pred_rf=tail(as.vector(exp(predict(rf_mod, newdata = df_tree_ts))),(7*24-1))

plot(df_plot_5b$date_hour, 
     df_plot_5b$y_actual, 
     pch = 16,
     lwd = 2, 
     cex = 0.5, 
     type = 'l', 
     col = colors[4], 
     ylab = "exp(logrides)",
     xlab = "Date",
     ylim = c(0,450))
lines(df_plot_5b$date_hour, 
      y_pred_rf, 
      col = colors[2], 
      lwd = 2)
legend(x = "topleft", 
       inset=.05, 
       legend = c("Actual", "Predict"),  
       lty = c(1, 1), 
       lwd = c(2, 2), 
       pch = c(NA, NA),
       col = c(colors[4], colors[2]))

```


```{r random-forest-experiments-1}
# Experimenting with fewer trees and full sample size

# Trees=250
rf_mod_tree250 = randomForest(y_tr_l ~ ., data = df_tree_tr, 
                         importance = TRUE, ntree = 250)
summary(rf_mod_tree250)

# Computing the test and training RMSE
tr_2=round((rmse_foo(rf_mod_tree250, df_tree_tr, y_tr_l)),4)
message("The training RMSE, tree=250: ", tr_2)

ts_2=round((rmse_foo(rf_mod_tree250, df_tree_ts, y_ts_l)),4)
message("The test RMSE, tree=250: ", ts_2)

# Trees=120
rf_mod_tree120 = randomForest(y_tr_l ~ ., data = df_tree_tr, 
                         importance = TRUE, ntree = 120)

# Computing the test and training RMSE
tr_3=round((rmse_foo(rf_mod_tree120, df_tree_tr, y_tr_l)),4)
message("The training RMSE, tree=120 ", tr_3)

ts_3=round((rmse_foo(rf_mod_tree120, df_tree_ts, y_ts_l)),4) 
message("The test RMSE, tree=120 ", ts_3)
```


```{r random-forest-experiments-2}
# Experimenting with full trees but fewer sample sizes
 
# Sampsize=3000
rf_mod_3000 = randomForest(y_tr_l ~ ., data = df_tree_tr, sampsize=3000,
                         importance = TRUE, ntrees = 500)
# Computing the test and training RMSE
tr_4=round((rmse_foo(rf_mod_3000, df_tree_tr, y_tr_l)),4)
message("The test RMSE, sample=3000 ", tr_4)

ts_4=round((rmse_foo(rf_mod_3000, df_tree_ts, y_ts_l)),4) 
message("The test RMSE, sample=3000 ", ts_4)


# Sampsize=1000
rf_mod_1000 = randomForest(y_tr_l ~ ., data = df_tree_tr, sampsize=1000,
                         importance = TRUE, ntree = 500)

# Computing the test and training RMSE
tr_5=round((rmse_foo(rf_mod_1000, df_tree_tr, y_tr_l)),4)
message("The test RMSE, sample=1000  ", tr_5)
#Test RMSE
ts_5=round((rmse_foo(rf_mod_1000, df_tree_ts, y_ts_l)),4)
message("The test RMSE, sample=1000  ", ts_5)

```


Is it better in terms of accuracy to train many trees, each with a smaller sample size, or to train fewer trees each with the full training set?

```{r random-forest-combine-results}
# Create a dataframe with all RMSE

rf_df = data.frame(
  Model = c("Default", "Trees_250", "Tress_120",  "Samp_3000",  "Samp_6000"),
  Number=c("1", "2", "3",  "4",  "5"),
  Train_rmse = c(tr_1, tr_2, tr_3,  tr_4, tr_5),
  Test_rmse = c(ts_1, ts_2, ts_3,  ts_4, ts_5)
)
rf_df

# Plotting the results
plot(rf_df$Number, rf_df$Train_rmse, pch = 20, cex = 0.5, ylim=c(0, 0.5),
     type="l", col = colors[4], xlab="Model", ylab="RMSE")
lines(rf_df$Number, rf_df$Test_rmse, col = colors[2], lwd = 1)

legend(x = "bottomright", inset=.05, legend = c("Train", "Test"),  
       lty = c(1, 1), lwd = c(0.5, 0.5), pch = c(NA, NA),
       col = c(colors[3], colors[2]))

```


It seems that in terms of the prediction accuracy it is better to train fewer trees each with the full training set than many trees, each with a smaller sample size.




#### Problem 8 - XGboost
Repeat Problem 6, this time using the xgboost regression model from the `xgboost` package. Use the argument nrounds = 25 when reporting the results, which means that the process iterates 25 times, but feel free to experiment with the settings to see the effect of changing them.

```{r xgboost}
set.seed(123342)

# Preparing the data
train_8 = data.matrix(lag_tr)
test_8 = data.matrix(lag_ts)

xgb_train = xgb.DMatrix(data = train_8, label = y_tr_l)
xgb_test = xgb.DMatrix(data = test_8, label = y_ts_l)

xgb_mod = xgboost(data = xgb_train,  nrounds = 25)
print(xgb_mod)

# Computing RMSE
tr_xg=round((rmse_foo(xgb_mod, xgb_train, y_tr_l)),4) 
message("The traing RMSE is ", tr_xg)

ts_xg=round((rmse_foo(xgb_mod, xgb_test, y_ts_l)),4) 
message("The test RMSE is", ts_xg)

# Plotting the actual observations versus the predicted for the last week
y_pred_8=tail(as.vector(exp(predict(xgb_mod, newdata = xgb_test))),(7*24-1))

plot(df_plot_5b$date_hour, 
     df_plot_5b$y_actual, 
     pch = 16,
     lwd = 2, 
     cex = 0.5, 
     type = 'l', 
     col = colors[4], 
     ylab = "exp(logrides)",
     xlab = "Date",
     ylim = c(0,450))
lines(df_plot_5b$date_hour, 
      y_pred_8, 
      col = colors[2], 
      lwd = 2)
legend(x = "topleft", 
       inset=.05, 
       legend = c("Actual", "Predict"),  
       lty = c(1, 1), 
       lwd = c(2, 2), 
       pch = c(NA, NA),
       col = c(colors[4], colors[2]))

```

```{r}
# Comparisons of the models, exercises 5-8.

# Create a dataframe with all RMSE in problems 5-8

df_all = data.frame(
  Model = c("Ex5", "Ex6",  "Ex7",  "Ex8"),
  Number=c( Number=c( "5", "6",  "7",  "8")),
  Train_rmse = c(rmse_tr_5, rmse_tr_6,  tr_1, tr_xg),
  Test_rmse = c(rmse_ts_5, rmse_ts_6,  ts_1, ts_xg)
)

df_all

plot(df_all$Number, df_all$Train_rmse, pch = 20, cex = 0.5, ylim=c(0,1),
     type="l", col = colors[4], xlab="Model", ylab="RMSE")
lines(df_all$Number, df_all$Test_rmse, col = colors[2], lwd = 1)

legend(x = "bottomright", inset=.05, legend = c("Train", "Test"),  
       lty = c(1, 1), lwd = c(0.5, 0.5), pch = c(NA, NA),
       col = c(colors[3], colors[2]))

```


It seems that the ensemble methods (random forest and XGboost) perform better in terms of the prediction accuracy than both the L1-regularized regression and regression tree model.